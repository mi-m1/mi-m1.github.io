---
title: "Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings"
collection: publications
permalink: /publication/2023-12-01-babylm
excerpt: 'This paper presents our findings for the
BabyLM Challenge (Warstadt et al., 2023). Our
exploration is inspired by vanilla curriculum
learning (Bengio et al., 2009) and we explored
the effect of linguistic complexity in forming
the best curriculum for pre-training. In particular, we explore curriculum formations based
on dependency-based measures (dependents
per token, average dependency distance) and
lexical-based measures (rarity, density, dispersion and diversity). We found that, overall,
models pretrained using curriculum learning
were able to beat the performance of a noncurriculum learning pre-trained model. Furthermore, we notice using different linguistic
metric for measuring complexity lead to advantageous performance for some tasks, but not all.
We share our results and analysis in the hope
that it can provide beneficial insights for future
work'
date: 2023-12-06
venue: 'Journal 1'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://aclanthology.org/2023.conll-babylm.23.pdf'
citation: 'Maggie Mi. 2023. Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 269â€“278, Singapore. Association for Computational Linguistics.'
---

The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font.